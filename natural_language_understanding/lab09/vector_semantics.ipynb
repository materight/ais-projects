{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Vector Semantics\n",
    "- Evgeny A. Stepanov\n",
    "- stepanov.evgeny.a@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "*Recommended Reading*:\n",
    "- Dan Jurafsky and James H. Martin. [__Speech and Language Processing__ (SLP)](https://web.stanford.edu/~jurafsky/slp3/) (3rd ed. draft)\n",
    "\n",
    "*Notebook Covers Material of*:\n",
    "- [SLP](https://web.stanford.edu/~jurafsky/slp3/6.pdf) Chapter 6: Vector Semantics and Embeddings\n",
    "\n",
    "__Requirements__\n",
    "\n",
    "- spaCy\n",
    "- [gensim](https://radimrehurek.com/gensim/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Words as Vectors (Embeddings)\n",
    "- Word embeddings is the process by which words are transformed into vectors of (real) numbers.\n",
    "- Definition of meaning by distributional similarity / usage: similar words are close in \"space\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### One-Hot Encoding\n",
    "- sparse vectors\n",
    "- most basic way to turn a token into a vector\n",
    "- method\n",
    "    - associate a unique integer index with every word in a vocabulary of size $V$\n",
    "    - turn this integer index $i$ into a binary vector of size $V$ (i.e. the size of the vocabulary)\n",
    "    - the vector has all values `0` except for the $i$th entry, which is `1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Co-Occurence Matrices and Word as Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Term-Document Matrix\n",
    "- could be used to represent words, where dimension are documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### TF-IDF\n",
    "- sparse vectors\n",
    "- generally used to represent documents, where dimensions are words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### TF: Term Frequency\n",
    "$$\\text{tf}_{t,d} = \\text{count}(t,d)$$\n",
    "$$\\text{tf}_{t,d} = \\log_{10}(\\text{count}(t,d) + 1)$$\n",
    "\n",
    "`+1` is because log of 0 is undefined.\n",
    "\n",
    "Alternatively:\n",
    "\n",
    "$$\\text{tf}_{t,d} = \n",
    "\\begin{cases}\n",
    "1 + \\log_{10}(\\text{count}(t,d)), & \\text{if count}(t,d) > 0\\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### IDF: Inverse Document Frequency\n",
    "\n",
    "$$\\text{idf}_t = \\frac{N}{\\text{df}_t}$$\n",
    "\n",
    "Usually in log space, like term frequency.\n",
    "\n",
    "$$\\text{idf}_t = \\log_{10}(\\frac{N}{\\text{df}_t})$$\n",
    "\n",
    "- $\\text{df}_t$ is the number of documents in which term $t$ occurs\n",
    "- $N$ is the total number of documents in the collection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The __tf-idf__ weighted value $w_{t,d}$ for word $t$ in document $d$ is the combination of $\\text{tf}_{t,d}$ and $\\text{idf}_t$:\n",
    "\n",
    "$$w_{t,d} = \\text{tf}_{t,d} \\times \\text{idf}_t$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Term-Term Matrix\n",
    "- a.k.a. \"word-word\" or \"word-context\" matrix\n",
    "- words are represented by a function of the counts of nearby words \n",
    "- size $|V| \\times |V|$, where $V$ is the vocabulary size\n",
    "    - usually context is taken to be a document or words in a window around the target word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pointwise Mutual Information (PMI) and Positive Pointwise Mutual Information (PPMI)\n",
    "- used for term-term matrices\n",
    "- \"the best way to weigh the association between two words is to ask how much more the two words co-occur in our corpus than we would have a priori expected them to appear by chance.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Pointwise Mutual Information (PMI)\n",
    "- a measure of how often two events $x$ and $y$ occur, compared with what we would expect if they were independent:\n",
    "\n",
    "$$I(x, y) = \\log_2 \\frac{P(x, y)}{P(x)P(y)}$$\n",
    "\n",
    "\n",
    "The pointwise mutual information between a target word $w$ and a context word $c$ is defined as:\n",
    "\n",
    "$$\\text{PMI}(w, c) = \\log_2 \\frac{P(w, c)}{P(w)P(c)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Positive Pointwise Mutual Information (PMI)\n",
    "- PMI values range from negative to positive infinity.\n",
    "- negative PMI values (which imply things are co-occurring less often than we would expect by chance) tend to be unreliable\n",
    "- it is more common to use Positive PMI (called PPMI) which replaces all negative PMI values with zero\n",
    "\n",
    "$$\\text{PPMI}(w, c) = \\max(\\log_2 \\frac{P(w, c)}{P(w)P(c)}, 0)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### PPMI Matrix\n",
    "To get a PPMI matrix from a co-occurrence matrix $F$, where $W$ rows are words and $C$ columns are contexts, and $f_{ij}$ is the number of times word $w_i$ appears in context $c_j$ (i.e. value of the cell).\n",
    "\n",
    "$$P(w,c) = \\frac{f_{ij}}{\\sum_{i=1}^W \\sum_{j=1}^C f_{ij}}$$\n",
    "\n",
    "$$P(w) = \\frac{\\sum_{j=1}^C f_{ij}}{\\sum_{i=1}^W \\sum_{j=1}^C f_{ij}}$$\n",
    "\n",
    "$$P(c) = \\frac{\\sum_{i=1}^W f_{ij}}{\\sum_{i=1}^W \\sum_{j=1}^C f_{ij}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- PMI has the problem of being biased toward infrequent events: very rare words tend to have very high PMI values.\n",
    "- Thus, $P(c)$ is computed as $P_{\\alpha}(c)$ that raises the probability of the context word to the power of $\\alpha$ (e.g. $0.75$)\n",
    "    - Alternative is Laplace smoothing\n",
    "\n",
    "$$\\text{PPMI}_{\\alpha}(w, c) = \\max(\\log_2 \\frac{P(w, c)}{P(w)P_{\\alpha}(c)}, 0)$$\n",
    "\n",
    "$$P_{\\alpha}(c) = \\frac{\\text{count}(c)^{\\alpha}}{\\sum_{c}\\text{count}(c)^{\\alpha}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Building Co-Occurence Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# let's define a function to build vocabulary\n",
    "def get_vocab(samples):\n",
    "    vocab = set()\n",
    "    for s in samples:\n",
    "        words = s if type(s) is list else s.split()\n",
    "        vocab = vocab.union(set(words))\n",
    "    return sorted(list(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cooc_matrix(samples, vocab):\n",
    "    m = np.zeros((len(vocab), len(vocab)))\n",
    "    # let's co-occurence be document level (i.e. sentence)\n",
    "    for s in samples:\n",
    "        for w1 in s:  # rows\n",
    "            for w2 in s:  # columns\n",
    "                i = vocab.index(w1)\n",
    "                j = vocab.index(w2)\n",
    "                m[i][j] += 1 \n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['France', 'Italy', 'Paris', 'Rome', 'capital', 'is', 'of', 'the']\n",
      "[[1. 0. 1. 0. 1. 1. 1. 1.]\n",
      " [0. 1. 0. 1. 1. 1. 1. 1.]\n",
      " [1. 0. 1. 0. 1. 1. 1. 1.]\n",
      " [0. 1. 0. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 2. 2. 2. 2.]\n",
      " [1. 1. 1. 1. 2. 2. 2. 2.]\n",
      " [1. 1. 1. 1. 2. 2. 2. 2.]\n",
      " [1. 1. 1. 1. 2. 2. 2. 2.]]\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    \"the capital of France is Paris\", \n",
    "    \"Rome is the capital of Italy\",\n",
    "]\n",
    "\n",
    "vocab = get_vocab(data)\n",
    "print(vocab)\n",
    "cm = cooc_matrix([s.split() for s in data], vocab)\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. 1. 1. 1. 1. 1.]\n",
      "[1. 0. 1. 0. 1. 1. 1. 1.]\n",
      "[1. 1. 1. 1. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "print(cm[vocab.index('Rome')])\n",
    "print(cm[vocab.index('Paris')])\n",
    "print(cm[vocab.index('is')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise\n",
    "- Extend the co-occurence matrix computation to allow specifying window of context (as tuple for previous and next words)\n",
    "- Define a funtion to compute PPMI on co-occurence matrix (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Vector Similarity\n",
    "- two words are similar in meaning if their context __vectors__ are similar\n",
    "- __Cosine similarity__ measures the similarity between two vectors of an __inner product space__. It is measured by the cosine of the angle between two vectors and determines whether two vectors are pointing in roughly the same direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dot Product\n",
    "\n",
    "- dot product (inner product)\n",
    "\n",
    "$$\\vec{v}\\cdot\\vec{w} = \\sum^N_{i=1}v_i w_i = v_1 w_1 + v_2 w_2 + ... + v_N w_N$$\n",
    "\n",
    "- vector length (L2 norm $||v||_2$)\n",
    "\n",
    "$$|\\vec{v}| = \\sqrt{\\sum^N_{i=1} v_i^2}$$ \n",
    "\n",
    "$$ |\\vec{v}| = \\sqrt{\\vec{v}\\cdot\\vec{v}} = \\sqrt{\\sum^N_{i=1} v_i v_i} = \\sqrt{\\sum^N_{i=1} v_1 v_1 + v_2 v_2 + ... + v_N v_N}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cosine Similarity\n",
    "\n",
    "- L2 normalized dot product of 2 vectors\n",
    "    - $\\theta$ is the angle between $\\vec{v}$ and $\\vec{w}$\n",
    "\n",
    "$$\\vec{v}\\cdot\\vec{w} = |\\vec{v}||\\vec{w}|\\cos\\theta$$\n",
    "\n",
    "$$\\cos\\theta = \\frac{\\vec{v}\\cdot\\vec{w}}{|\\vec{v}||\\vec{w}|}$$\n",
    "\n",
    "$$\\text{CosSim}(\\vec{v},\\vec{w}) = \\frac{\\vec{v}\\cdot\\vec{w}}{|\\vec{v}||\\vec{w}|} = \\frac{\\sum^N_{i=1}v_i w_i}{\\sqrt{\\sum^N_{i=1} v_i^2} \\sqrt{\\sum^N_{i=1} w_i^2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Cosine Distance\n",
    "$$\\text{Cosine Distance}(\\vec{v}, \\vec{w}) = 1 - \\text{Cosine Similarity}(\\vec{v}, \\vec{w})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercises\n",
    "- Implement one-hot encoding (binary vecorization)\n",
    "    - takes vocabulary and a sentence as arguments (lists of words)\n",
    "    - outputs numpy vector (`ndarray`)\n",
    "- Implement a function to compute __cosine similarity__ using `numpy` methods\n",
    "    - `np.dot`\n",
    "    - `np.sqrt`\n",
    "- Using the defined functions\n",
    "    - vectorize the sentences:\n",
    "        - \"the capital of France is Paris\"\n",
    "        - \"Rome is the capital of Italy\"\n",
    "    - compute cosine similarity between them\n",
    "    - compare similarity values to the cosine similarity using the ouput of (`scipy.spatial.distance.cosine`)\n",
    "        - i.e. use *distance* to compute *similarity*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training Word Embeddings with gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Word2Vec\n",
    "- dense vectors\n",
    "- representation is created by training a classifier to distinguish nearby and far-away words\n",
    "- Variants\n",
    "    - SKIP-GRAM\n",
    "    - CBOW\n",
    "- Refer to [documentation](https://radimrehurek.com/gensim/models/word2vec.html) for details\n",
    "- [Tutorial](https://rare-technologies.com/word2vec-tutorial/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# training the model\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(sentences=[d.split() for d in data], vector_size=10, window=5, min_count=1, workers=4)\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# loading the model\n",
    "model = Word2Vec.load(\"word2vec.model\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# getting word vectors\n",
    "print(model.wv['Rome'])\n",
    "# getting most similar\n",
    "print(model.wv.most_similar('Rome', topn=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pre-Trained Embeddings\n",
    "- Training embeddings is computationally expensive\n",
    "- Many pre-trained models are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "# Show all available models in gensim-data\n",
    "print(list(gensim.downloader.info()['models'].keys()))\n",
    "# Download the 'word2vec-google-news-300' embeddings\n",
    "# w2v = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word Embeddings in spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "> To make them compact and fast, spaCy's small pipeline packages (all packages that end in `sm`) don't ship with word vectors, and only include context-sensitive tensors. This means you can still use the `similarity()` methods to compare documents, spans and tokens -- but the result won't be as good, and individual tokens won't have any vectors assigned. So in order to use real word vectors, you need to download a larger pipeline package:\n",
    "\n",
    "> `python -m spacy download en_core_web_lg`\n",
    "\n",
    "> Pipeline packages that come with built-in word vectors make them available as the `Token.vector` attribute. `Doc.vector` and `Span.vector` will default to an __average of their token vectors__. You can also check if a token has a vector assigned, and get the L2 norm, which can be used to normalize vectors.\n",
    "\n",
    "> Each `Doc`, `Span`, `Token` and `Lexeme` comes with a `.similarity` method that lets you compare it with another object, and determine the similarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy.cli.download('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Accessing Embedding Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string: Rome\n",
      "vector dimension: 300\n",
      "spacy vector norm: 7.191654\n",
      "numpy vector norm: 7.191654\n",
      "numpy linalg norm: 7.191654\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "txt = 'Rome is the capital of Italy'\n",
    "doc = nlp(txt)\n",
    "\n",
    "tok = doc[0]  # let's get Rome\n",
    "\n",
    "print(\"string:\", tok.text)\n",
    "# print(\"vector:\", tok.vector)\n",
    "print(\"vector dimension:\", len(tok.vector))\n",
    "print(\"spacy vector norm:\", tok.vector_norm)\n",
    "print(\"numpy vector norm:\", np.sqrt(np.dot(tok.vector, tok.vector)))\n",
    "print(\"numpy linalg norm:\", np.linalg.norm(tok.vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris\n",
      "spacy CosSim(Rome, Paris): 0.58241165\n",
      "scipy CosSim(Rome, Paris): 0.5824115872383118\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# let's get Paris & compare its vector to rome\n",
    "paris = nlp('Paris')[0]\n",
    "print(paris.text)\n",
    "\n",
    "print(\"spacy CosSim({}, {}):\".format(tok.text, paris.text), tok.similarity(paris))\n",
    "print(\"scipy CosSim({}, {}):\".format(tok.text, paris.text), 1 - cosine(tok.vector, paris.vector))\n",
    "# print(\"_our_ CosSim({}, {}):\".format(tok.text, paris.text), cossim(tok.vector, paris.vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Evaluation: Analogy Task\n",
    "In the word analogy task, we complete the sentence of the form\n",
    "\n",
    "\"$w_1$ is to $w_2$ as $w_3$ is to $w4$\", where $w_4$ is a blank. \n",
    "\n",
    "For instance:\n",
    "\n",
    "\"*man* is to *woman* as *king* is to **__**\", and our goal is to guess the missing word (*queen*)\n",
    "\n",
    "The task is approached using cosine similarity between vector differences: \n",
    "\n",
    "$$\\vec{w_2} - \\vec{w_1} \\approx \\vec{w_4} - \\vec{w_3}$$\n",
    "\n",
    "$$\\vec{w_4} \\approx = \\vec{w_3} + \\vec{w_2} - \\vec{w_1}$$\n",
    "\n",
    "$$w = \\arg\\max_{w \\in V}(\\vec{w} \\cdot (\\vec{w_3} + \\vec{w_2} - \\vec{w_1}))$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$w = \\arg\\max_{w \\in V}\\text{CosSim}(\\vec{w_2} - \\vec{w_1}, \\vec{w} - \\vec{w_3})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Analogy using Most Similar\n",
    "> For each of the given vectors, find the `n` most similar entries to it by cosine. \n",
    "Queries are by vector. Results are returned as a (`keys`, `best_rows`, `scores`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def analogy_spacy(w1, w2, w3):\n",
    "    v1 = nlp.vocab[w1].vector\n",
    "    v2 = nlp.vocab[w2].vector\n",
    "    v3 = nlp.vocab[w3].vector\n",
    "    \n",
    "    # relation vector\n",
    "    rv = v3 + v2 - v1\n",
    "    \n",
    "    # n=1 & sorted by default\n",
    "    ms = nlp.vocab.vectors.most_similar(np.asarray([rv]), n=10)\n",
    "    print(ms)\n",
    "    \n",
    "    # getting words & scores\n",
    "    for i, key in enumerate(ms[0][0]):\n",
    "        print(nlp.vocab.strings[key], ms[2][0][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[13176088972490086564, 14826469074451677028,  7464393751932445219,\n",
      "         7102492827649024548, 10168488388102651113,  4176741725343376093,\n",
      "         5247273317732208552, 11742085837932180620, 12278543830867659210,\n",
      "         4527521648030784477]], dtype=uint64), array([[391588,   2183,   3150,  27270,   5310,   6026,  59856,  94889,\n",
      "         11900,   7474]], dtype=int32), array([[0.8024, 0.8024, 0.8024, 0.8024, 0.7881, 0.7881, 0.7881, 0.6401,\n",
      "        0.6401, 0.6401]], dtype=float32))\n",
      "KIng 0.8024\n",
      "King 0.8024\n",
      "king 0.8024\n",
      "KING 0.8024\n",
      "Queen 0.7881\n",
      "queen 0.7881\n",
      "QUEEN 0.7881\n",
      "PRINCE 0.6401\n",
      "prince 0.6401\n",
      "Prince 0.6401\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(analogy_spacy('man', 'woman', 'king'))\n",
    "# expected output ('Queen', 0.7881)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0.7606 france -inf\n",
      "4 0.7606 France 0.7606\n",
      "5 0.7606 FRANCE 0.7606\n",
      "9 0.6176 Europe 0.7606\n",
      "('france', 0.7606)\n"
     ]
    }
   ],
   "source": [
    "# print(analogy_spacy('Rome', 'Italy', 'Paris'))\n",
    "# ('france', 0.7606)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise\n",
    "Implement analogy compuatation function that\n",
    "- takes 3 words of the analogy task as an input\n",
    "- outputs the word 4\n",
    "- make use of spacy vectors\n",
    "\n",
    "version 2:\n",
    "- implement without using spacy's `most_similar`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
