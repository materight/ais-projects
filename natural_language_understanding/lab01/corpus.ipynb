{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Corpus and Lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- __author__: Evgeny A. Stepanov\n",
    "- __e-mail__: stepanov.evgeny.a@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Dan Jurafsky and James H. Martin's __Speech and Language Processing__ ([3rd ed. draft](https://web.stanford.edu/~jurafsky/slp3/)) is advised for reading. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Section *Corpora and Counting* covers some concepts of *Chapter 2: \"Regular Expressions, Text Normalization, Edit Distance\"*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Requirements__\n",
    "\n",
    "- [NL2SparQL4NLU](https://github.com/esrel/NL2SparQL4NLU) dataset\n",
    "\n",
    "    - run `git clone https://github.com/esrel/NL2SparQL4NLU.git`\n",
    "    \n",
    "- [spaCy](https://spacy.io/)\n",
    "    - run `pip install spacy`\n",
    "    - run `python -m spacy download en` to install English language model\n",
    "    \n",
    "- [NLTK](http://www.nltk.org/)\n",
    "    - run `pip install nltk`\n",
    "\n",
    "- [scikit-learn](https://scikit-learn.org/)\n",
    "    - run `pip install scikit-learn`\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Alternative Corpora__\n",
    "\n",
    "Use __only__ if you know how to work with JSON!\n",
    "\n",
    "- https://github.com/howl-anderson/ATIS_dataset\n",
    "- https://github.com/sebischair/NLU-Evaluation-Corpora\n",
    "- https://github.com/sonos/nlu-benchmark\n",
    "- https://github.com/clinc/oos-eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Corpora and Counting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.1. Corpus\n",
    "\n",
    "[Corpus](https://en.wikipedia.org/wiki/Text_corpus) is a collection of written or spoken texts that is used for language research. Before doing anything with a corpus we need to know its properties:\n",
    "\n",
    "__Corpus Properties__:\n",
    "- *Format* -- how to read/load it?\n",
    "- *Language* -- which tools/models can I use?\n",
    "- *Annotation* -- what it is intended for?\n",
    "- *Split* for __Evaluation__: (terminology varies from source to source)\n",
    "\n",
    "| Set         | Purpose                                       |\n",
    "|:------------|:----------------------------------------------|\n",
    "| Training    | training model, extracting rules, etc.        |\n",
    "| Development | tuning, optimization, intermediate evaluation |\n",
    "| Test        | final evaluation (remains unseen)             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### NL2SparQL4NLU\n",
    "\n",
    "- __Format__:\n",
    "\n",
    "    - Utterance (sentence) per line\n",
    "    - Tokenized\n",
    "    - Lowercased\n",
    "\n",
    "- __Language__: English monolingual\n",
    "\n",
    "- __Annotation__: None (for now)\n",
    "\n",
    "- __Split__: training & test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise\n",
    "\n",
    "- define a function to load a corpus into a list-of-lists\n",
    "\n",
    "- load `NL2SparQL4NLU/dataset/NL2SparQL4NLU.train.utterances.txt`\n",
    "- print first `2` tokens of the first `10` utterances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "trn_path='NL2SparQL4NLU/dataset/NL2SparQL4NLU.train.utterances.txt'\n",
    "tst_path='NL2SparQL4NLU/dataset/NL2SparQL4NLU.test.utterances.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "who\tplays\nshow\tcredits\nwho\twas\nfind\tthe\nwho\tplayed\nwho\twas\nwho\tplayed\nwho\twas\nfind\tthe\ncast\tand\n"
     ]
    }
   ],
   "source": [
    "def load(path, is_swl=False):\n",
    "    with open(path) as file:\n",
    "        return [([word for word in line.strip('\\n').split(' ')] if not is_swl else line.strip('\\n')) for line in file]\n",
    "\n",
    "trn = load(trn_path)\n",
    "tst = load(tst_path)\n",
    "\n",
    "for u in trn[:10]: print(f'{u[0]}\\t{u[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.2. Corpus Descriptive Statistics (Counting)\n",
    "\n",
    "*Corpus* description in terms of:\n",
    "\n",
    "- total number of tokens\n",
    "- total number of utterances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise\n",
    "\n",
    "- define a function to compute corpus descriptive statistics -- number of utterance and token counts\n",
    "- compute the statistics for the __training__ and __test__ sets of NL2SparQL4NLU dataset. \n",
    "- compare the computed statistics with the reference values below.\n",
    "\n",
    "\n",
    "| Metric           | Train  | Test   |\n",
    "|------------------|-------:|-------:|\n",
    "| Total Tokens     | 21,453 |  7,117 |\n",
    "| Total Utterances |  3,338 |  1,084 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Metric\t\tTrain\tTest\nTotal Tokens:\t21453\t7117\nTotal Utter.:\t3338\t1084\n"
     ]
    }
   ],
   "source": [
    "from functools import *\n",
    "\n",
    "def tot_tokens(data):\n",
    "    return reduce(lambda sum, u: sum + len(u), data, 0)\n",
    "\n",
    "print('Metric\\t\\tTrain\\tTest')\n",
    "print(f'Total Tokens:\\t{tot_tokens(trn)}\\t{tot_tokens(tst)}')\n",
    "print(f'Total Utter.:\\t{len(trn)}\\t{len(tst)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise\n",
    "\n",
    "- define a function to compute average number of tokens per utterance statistic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Metric\t\tTrain\tTest\nAvg Tokens:\t6.43\t6.57\n"
     ]
    }
   ],
   "source": [
    "def avg_tokens(data):\n",
    "    return tot_tokens(data) / len(data)\n",
    "\n",
    "print('Metric\\t\\tTrain\\tTest')\n",
    "print(f'Avg Tokens:\\t{avg_tokens(trn):.2f}\\t{avg_tokens(tst):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Lexicon\n",
    "\n",
    "[Lexicon](https://en.wikipedia.org/wiki/Lexicon) is the *vocabulary* of a language. In linguistics, a lexicon is a language's inventory of lexemes.\n",
    "\n",
    "Linguistic theories generally regard human languages as consisting of two parts: a lexicon, essentially a catalog of a language's words; and a grammar, a system of rules which allow for the combination of those words into meaningful sentences. \n",
    "\n",
    "*Lexicon (or Vocabulary) Size* is one of the statistics reported for corpora. While *Word Count* is the number of __tokens__, *Lexicon Size* is the number of __types__ (unique words).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.1. Lexicon Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Exercise\n",
    "\n",
    "- define a function to compute a lexicon from corpus in a list-of-lists format\n",
    "    - sort the list alphabetically\n",
    "    \n",
    "- compute the lexicon of the training set of NL2SparQL4NLU dataset\n",
    "- compare its size to the reference value below.\n",
    "\n",
    "| Metric       | Value |\n",
    "|--------------|------:|\n",
    "| Lexicon Size | 1,729 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Metric\t\tTrain\tTest\nLexicon Size:\t1729\t1039\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def lexicon(data):\n",
    "    return np.unique(np.concatenate(data)).tolist()\n",
    "\n",
    "print('Metric\\t\\tTrain\\tTest')\n",
    "print(f'Lexicon Size:\\t{len(lexicon(trn))}\\t{len(lexicon(tst))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.2. Frequency List\n",
    "\n",
    "In Natural Language Processing (NLP), [a frequency list](https://en.wikipedia.org/wiki/Word_lists_by_frequency) is a sorted list of words (word types) together with their frequency, where frequency here usually means the number of occurrences in a given corpus, from which the rank can be derived as the position in the list.\n",
    "\n",
    "What is a \"word\"?\n",
    "\n",
    "- case sensitive counts\n",
    "- case insensitive counts (our corpus is lowercased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise\n",
    "\n",
    "- define a function to compute a frequency list for a corpus\n",
    "- compute frequency list for the training set of NL2SparQL4NLU dataset\n",
    "- report `5` most frequent words (use can use provided `nbest` function to get a dict of top N items)\n",
    "- compare the frequencies to the reference values below\n",
    "\n",
    "| Word   | Frequency |\n",
    "|--------|----------:|\n",
    "| the    |     1,337 |\n",
    "| movies |     1,126 |\n",
    "| of     |       607 |\n",
    "| in     |       582 |\n",
    "| movie  |       564 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def nbest(d, n=1):\n",
    "    \"\"\"\n",
    "    get n max values from a dict\n",
    "    :param d: input dict (values are numbers, keys are stings)\n",
    "    :param n: number of values to get (int)\n",
    "    :return: dict of top n key-value pairs\n",
    "    \"\"\"\n",
    "    return dict(sorted(d.items(), key=lambda item: item[1], reverse=True)[:n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Word\tFrequency\nthe\t1337\nmovies\t1126\nof\t607\nin\t582\nmovie\t564\n"
     ]
    }
   ],
   "source": [
    "def lexicon_freq(data):\n",
    "    return dict(zip(*np.unique(np.concatenate(data), return_counts=True)))\n",
    "\n",
    "print('Word\\tFrequency')\n",
    "for w, f in nbest(lexicon_freq(trn), n=5).items(): print(f'{w}\\t{f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.3. Lexicon Operations\n",
    "\n",
    "It is common to process the lexicon according to the task at hand (not every transformation makes sense for all tasks). The common operations are removing words by frequency (minimum or maximum, i.e. *Frequency Cut-Off*) and removing words for a specific lists (i.e. *Stop Word Removal*).\n",
    "\n",
    "In computing, [stop words](https://en.wikipedia.org/wiki/Stop_words) are words which are filtered out before or after processing of natural language data (text). Though \"stop words\" usually refers to the most common words in a language, there is no single universal list of stop words used by all natural language processing tools, and indeed not all tools even use such a list. Some tools specifically avoid removing these stop words to support phrase search.\n",
    "\n",
    "Any group of words can be chosen as the stop words for a given purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Frequency Cut-Off\n",
    "\n",
    "- define a function to compute a lexicon from a frequency list applying minimum and maximum frequency cut-offs\n",
    "\n",
    "    - use default values for min and max\n",
    "    \n",
    "- using frequency list for the training set of NL2SparQL4NLU dataset\n",
    "    \n",
    "    - compute lexicon applying:\n",
    "    \n",
    "        - minimum cut-off 2 (remove words that appear less than 2 times, i.e. remove [hapax legomena](https://en.wikipedia.org/wiki/Hapax_legomenon))\n",
    "        - maximum cut-off 100 (remove words that appear more that 100 times)\n",
    "        - both minimum and maximum thresholds together\n",
    "        \n",
    "    - report size for each comparing to the reference values in the table\n",
    "\n",
    "| Operation  | Min | Max | Size |\n",
    "|------------|----:|----:|-----:|\n",
    "| original   | N/A | N/A | 1729 |\n",
    "| cut-off    |   2 | N/A |  950 |\n",
    "| cut-off    | N/A | 100 | 1694 |\n",
    "| cut-off    |   2 | 100 |  915 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Operation\tMin\tMax\tSize\noriginal\tN/A\tN/A\t1729\ncut-off\t\t2\tN/A\t950\ncut-off\t\tN/A\t100\t1694\ncut-off\t\t2\t100\t915\n"
     ]
    }
   ],
   "source": [
    "def freq_cutoff(data, minf=None, maxf=None):\n",
    "    frequencies = lexicon_freq(data)\n",
    "    return [w for w, f in frequencies.items() if (minf is None or f >= minf) and (maxf is None or f <= maxf)]\n",
    "\n",
    "print('Operation\\tMin\\tMax\\tSize')\n",
    "print(f'original\\tN/A\\tN/A\\t{len(freq_cutoff(trn))}')\n",
    "print(f'cut-off\\t\\t2\\tN/A\\t{len(freq_cutoff(trn, minf=2))}')\n",
    "print(f'cut-off\\t\\tN/A\\t100\\t{len(freq_cutoff(trn, maxf=100))}')\n",
    "print(f'cut-off\\t\\t2\\t100\\t{len(freq_cutoff(trn, minf=2, maxf=100))}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Stop Word Removal\n",
    "\n",
    "- define a function to read/load a list of words in token-per-line format (i.e. lexicon)\n",
    "- load stop word list from `NL2SparQL4NLU/extras/english.stop.txt`\n",
    "- using Python's built it `set` [methods](https://docs.python.org/2/library/stdtypes.html#set):\n",
    "    \n",
    "    - define a function to compute overlap of two lexicons\n",
    "    - define a function to apply a stopword list to a lexicon\n",
    "\n",
    "- compare the 100 most frequent words in frequency list of the training set to the list of stopwords (report count)\n",
    "- apply stopword list to the lexicon of the training set\n",
    "- report size of the resulting lexicon comparing to the reference values.\n",
    "\n",
    "| Operation       | Size |\n",
    "|-----------------|-----:|\n",
    "| original        | 1729 |\n",
    "| no stop words   | 1529 |\n",
    "| top 100 overlap |   50 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "swl_path='NL2SparQL4NLU/extras/english.stop.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Operation\tSize\noriginal\t1729\nno stop words\t1529\n"
     ]
    }
   ],
   "source": [
    "def lexicon_overlap(lex1, lex2):\n",
    "    return list(set(lex1).intersection(set(lex2)))\n",
    "\n",
    "def apply_swl(lexicon, swl):\n",
    "    return list(set(lexicon) - set(swl))\n",
    "\n",
    "lex = lexicon(trn)\n",
    "swl = load(swl_path, is_swl=True)\n",
    "\n",
    "lexicon_overlap(lex, swl)\n",
    "apply_swl(lex, swl)\n",
    "\n",
    "print('Operation\\tSize')\n",
    "print(f'original\\t{len(lex)}')\n",
    "print(f'no stop words\\t{len(apply_swl(lex, swl))}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Exercise: Alternative Stop Words\n",
    "\n",
    "Compare the stop word list above to the stop word lists from the popular python libraries in terms of overlaps.\n",
    "(Use `set` `intersection`)\n",
    "\n",
    "- spaCy\n",
    "- NLTK\n",
    "- scikit-learn\n",
    "\n",
    "    \n",
    "For NLTK you need to download them first\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "spaCy: 326\n",
      "NLTK: 179\n",
      "sklearn: 318\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mdestro/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS as SPACY_STOP_WORDS\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as SKLEARN_STOP_WORDS\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "NLTK_STOP_WORDS = set(stopwords.words('english'))\n",
    "\n",
    "print('spaCy: {}'.format(len(SPACY_STOP_WORDS)))\n",
    "print('NLTK: {}'.format(len(NLTK_STOP_WORDS)))\n",
    "print('sklearn: {}'.format(len(SKLEARN_STOP_WORDS)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Basic Text Pre-processing\n",
    "\n",
    "Both frequency cut-off and stop word removal are frequently used text pre-processing steps. Depending on the application, there are several other common text pre-processing steps that are usually applied for tranforming text for Machine Learning tasks.\n",
    "\n",
    "__Text Normalization Steps__\n",
    "\n",
    "- removing extra white spaces\n",
    "\n",
    "- tokenization\n",
    "    - documents to sentences (sentence segmentation/tokenization)\n",
    "    - sentences to tokens\n",
    "\n",
    "- lowercasing/uppercasing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- removing punctuation\n",
    "\n",
    "- removing accent marks and other diacritics \n",
    "\n",
    "- removing stop words (see above)\n",
    "\n",
    "- removing sparse terms (frequency cut-off)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- number normalization\n",
    "    - numbers to words (i.e. `10` to `ten`)\n",
    "    - number words to numbers (i.e. `ten` to `10`)\n",
    "    - removing numbers\n",
    "\n",
    "- verbalization (specifically for speech applications)\n",
    "\n",
    "    - numbers to words\n",
    "    - expanding abbreviations (or spelling out)\n",
    "    - reading out dates, etc.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- [lemmatization](https://en.wikipedia.org/wiki/Lemmatisation)\n",
    "    - the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form.\n",
    "\n",
    "- [stemming](https://en.wikipedia.org/wiki/Stemming)\n",
    "    - the process of reducing inflected (or sometimes derived) words to their word stem, base or root form—generally a written word form.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.1. Tokenization and Lemmatization with spaCy\n",
    "\n",
    "The default spaCy NLP pipeline does several processing steps including __tokenization__, *part of speech tagging*, __lemmatization__, *dependency parsing* and *Named Entity Recognition* (ignore the ones in *italics* for today). \n",
    "\n",
    "\n",
    "SpaCy produces a `Doc` object that contains `Token`s. It is possible to access lemmatized form of a token using its `lemma_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['who', 'play', 'luke', 'on', 'star', 'war', 'new', 'hope']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "txt = 'who plays luke on star wars new hope'\n",
    "doc = nlp(txt)\n",
    "\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise\n",
    "\n",
    "- Lemmatize the dataset with spaCy\n",
    "- compute the lexicon of the training set of NL2SparQL4NLU dataset (or the one you have chosen)\n",
    "- compare its size to the \"raw\" counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.2. Stemming with NLTK\n",
    "\n",
    "SpaCy does not provide any stemming algorithms.\n",
    "NLTK, on the other hand, provides two algorithms [`Porter Stemmer`](https://tartarus.org/martin/PorterStemmer/) and [`Snowball Stemmer`](https://snowballstem.org/algorithms/) (a.k.a. `Porter2`). \n",
    "\n",
    "__Note__: Please read the original description of the algorithmsm, if you are interested.\n",
    "\n",
    "Since stemming works on token level, we need to provide tokens. Which we can obtain either from `spacy`'s `Doc` or just *whitespace tokenization*\n",
    "\n",
    "```python\n",
    "\n",
    "tokens = [token.text for token in doc]\n",
    "\n",
    "```\n",
    "\n",
    "or\n",
    "\n",
    "```python\n",
    "tokens = text.split()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['who', 'plays', 'luke', 'on', 'star', 'wars', 'new', 'hope']\n",
      "['who', 'play', 'luke', 'on', 'star', 'war', 'new', 'hope']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "txt = 'who plays luke on star wars new hope'\n",
    "tokens = txt.split()\n",
    "print(tokens)\n",
    "\n",
    "stems = [stemmer.stem(token) for token in tokens]\n",
    "print(stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise\n",
    "- Stem the dataset with NTLK\n",
    "- compute the lexicon of the training set of NL2SparQL4NLU dataset (or the one you have chosen)\n",
    "- compare its size to the \"raw\" and lemmatized counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}