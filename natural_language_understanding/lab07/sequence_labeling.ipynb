{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sequence Labeling with Markov Models\n",
    "- Evgeny A. Stepanov\n",
    "- stepanov.evgeny.a@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "*Recommended Reading*:\n",
    "- Dan Jurafsky and James H. Martin. [__Speech and Language Processing__ (SLP)](https://web.stanford.edu/~jurafsky/slp3/) (3rd ed. draft)\n",
    "- Steven Bird, Ewan Klein, and Edward Loper. [__Natural Language Processing with Python__ (NLTK)](https://www.nltk.org/book/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*Notebook Covers Material of*:\n",
    "- [SLP](https://web.stanford.edu/~jurafsky/slp3/8.pdf) Chapter 8: Part-of-Speech Tagging (HMMs)\n",
    "- [NLTK](https://www.nltk.org/book/ch05.html) \n",
    "    - Chapter 5: Part of Speech Tagging \n",
    "    - Chapter 7: Extracting Information from Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Requirements__\n",
    "\n",
    "- spaCy\n",
    "- [NLTK](https://www.nltk.org/)\n",
    "- [`conll.py`](https://github.com/esrel/LUS/) (in `src` folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sequence Labeling (Tagging)\n",
    "[Classification](https://en.wikipedia.org/wiki/Statistical_classification) is the problem of identifying to which of a set of categories (sub-populations) a new observation belongs, on the basis of a training set of data containing observations (or instances) whose category membership is known.\n",
    "\n",
    "[Sequence Labeling](https://en.wikipedia.org/wiki/Sequence_labeling) is a type of pattern recognition task that involves the algorithmic assignment of a categorical label to each member of a sequence of observed values. It is a sub-class of [structured (output) learning](https://en.wikipedia.org/wiki/Structured_prediction), since we are predicting a *sequence* object rather than a discrete or real value predicted in classification problems.\n",
    "\n",
    "- Can be treated as a set of independent classification tasks, one per member of the sequence;\n",
    "- Performance is generally improved by making the optimal label for a given element dependent on the choices of nearby elements;\n",
    "\n",
    "Due to the complexity of the model and the interrelations of predicted variables the process of prediction using a trained model and of training itself is often computationally infeasible and [approximate inference](https://en.wikipedia.org/wiki/Approximate_inference) and learning methods are used. \n",
    "\n",
    "[Markov Chain](https://en.wikipedia.org/wiki/Markov_chain) is a stochastic model used to describe sequences. It is the simplest [Markov Model](https://en.wikipedia.org/wiki/Markov_model). In order to make inference tractable, a process that generated the sequence is assumed to have [Markov Property](https://en.wikipedia.org/wiki/Markov_property), i.e. future states depend only on the current state, not on the events that occurred before it. (An [ngram](https://en.wikipedia.org/wiki/N-gram) [language model](https://en.wikipedia.org/wiki/Language_model) is a $(n-1)$-order Markov Model.) \n",
    "\n",
    "In Statical Language Modeling, we are modeling *observed sequences* represented as Markov Chains. Since the states of the process are *observable*, we only need to compute __transition probabilities__. \n",
    "\n",
    "In Sequence Labeling, we assume that *observed sequences* (__sentences__) have been generated by a Markov Process with *unobservable* (i.e. hidden) states (__labels__), i.e. [Hidden Markov Model](https://en.wikipedia.org/wiki/Hidden_Markov_model) (__HMM__). \n",
    "Since the states of the process are hidden and the output is observable, each state has a probability distribution over the possible output tokens, i.e. __emission probabilities__. \n",
    "\n",
    "Using these two probability distributions (__transition__ and __emission__), in sequence labeling, we are *inferring* the sequence of state transitions, given a sequence of observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Natural Language Processing (NLP) Tasks\n",
    "\n",
    "Below are some examples of NLP tasks that Sequence Labeling is applied to as one of the methods.\n",
    "\n",
    "The scenario when members of a sequence are mapped to higher order units (i.e. grouped together `[['a'],['b','c']]`) and assigned a category) is known as __shallow parsing__.\n",
    "\n",
    "- [Part-of-Speech Tagging](https://en.wikipedia.org/wiki/Part-of-speech_tagging)\n",
    "- [Shallow Parsing](https://en.wikipedia.org/wiki/Shallow_parsing) (Chunking)\n",
    "    - [Phrase Chunking](https://en.wikipedia.org/wiki/Phrase_chunking)\n",
    "    - [Named-Entity Recognition](https://en.wikipedia.org/wiki/Named-entity_recognition) \n",
    "    - [Semantic Role Labeling](https://en.wikipedia.org/wiki/Semantic_role_labeling)\n",
    "    - Dependency [Parsing](https://en.wikipedia.org/wiki/Parsing) \n",
    "    - Discourse Parsing\n",
    "    - (Natural/Spoken) __Language Understanding__: Concept Tagging/Entity Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The General Setting for Sequence Labeling\n",
    "\n",
    "- Create __training__ and __testing__ sets by tagging a certain amount of text by hand\n",
    "    - i.e. map each word in corpus to a tag\n",
    "- Train tagging model to extract generalizations from the annotated __training__ set\n",
    "- Evaluate the trained tagging model on the annotated __testing__ set\n",
    "- Use the trained tagging model too annotate new texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Markov Model Tagging\n",
    "Tagging is one of the tasks [Hidden Markov Models](https://en.wikipedia.org/wiki/Hidden_Markov_model) are used for.\n",
    "\n",
    "Given s word sequence $w_{1}^{n}$ the goal is to find the most probable tag sequence $t_{1}^{n}$. \n",
    "\n",
    "$$t_{1}^{n} = \\arg\\max\\limits_{t_{1}^{n}} p(t_{1}^{n} | w_{1}^{n})$$\n",
    "\n",
    "We assume that a tag sequence has generated the given sequence of words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Using __Bayes's Rule__ \n",
    "\n",
    "$$P(A|B)=\\frac{P(B|A)P(A)}{P(B)}$$\n",
    "\n",
    "Consequently, we compute:\n",
    "\n",
    "$$t_{1}^{n} = \\arg\\max\\limits_{t_{1}^{n}}\\frac{p(w_{1}^{n} | t_{1}^{n}) p(t_{1}^{n})}{p(w_{1}^{n})}$$\n",
    "\n",
    "Probability of a word sequence is the same for all tags, thus:\n",
    "\n",
    "$$t_{1}^{n} = \\arg\\max\\limits_{t_{1}^{n}} p(w_{1}^{n} | t_{1}^{n})p(t_{1}^{n})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Parameter Learning\n",
    "The parameter learning task in HMMs is to find, given an output sequence or a set of such sequences, the best set of *state transition* and *emission probabilities*. The task is usually to derive the [*maximum likelihood estimate*](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation) of the parameters of the HMM given the set of output sequences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Simplifying Assumptions\n",
    "\n",
    "- Probability of a word only depends on its own tag, not tags of other words in a sentence, thus:\n",
    "\n",
    "$$p(w_{1}^{n}|t_{1}^{n}) \\approx p(w_1|t_1)p(w_2|t_2) ... p(w_n|t_n)$$\n",
    "\n",
    "- Probability of a tag depends on previous N tags; i.e. Markov assumption (ngram), thus:\n",
    "\n",
    "$$p(t_{1}^{n}) \\approx \\prod_{i=1}^{n}{p(t_i | t_{i-n+1}^{i-1})}$$\n",
    "\n",
    "- The (first-order) Markov assumption (bigram):\n",
    "\n",
    "$$p(t_{1}^{n}) \\approx p(t_1|t_0) p(t_2|t_1) ... p(t_n|t_{n-1})$$\n",
    "- or:\n",
    "$$p(t_{1}^{n}) \\approx \\prod_{i=1}^{n}{p(t_i | t_{i-1})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Estimating Transition Probabilities from Data\n",
    "\n",
    "- *Transition Probabilities* $p(t_i | t_{i-n+1}^{i-1})$ is an ngram model, and it is estimated using the same recipe we use for ngram language modeling; but using tag ngrams instead of word-ngrams. \n",
    "- It is assumed that the set of states is *finite* and known (i.e. there is no unknown (or OOV) state).\n",
    "- The same principles of *smoothing* apply for ngrams of state transitions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "*Calculating Probability from Frequencies*\n",
    "\n",
    "Probabilities of ngrams can be computed *normalizing* frequency counts (*Maximum Likelihood Estimation*): dividing the frequency of an ngram sequence by the frequency of its prefix (*relative frequency*).\n",
    "\n",
    "N-gram   | Equation                      \n",
    ":--------|:------------------------------\n",
    "Unigram  | $$p(t_i) = \\frac{c(t_i)}{T}$$ \n",
    "Bigram   | $$p(t_i|t_{i-1}) = \\frac{c(t_{i-1},t_i)}{c(t_{i-1})}$$ \n",
    "Ngram    | $$p(t_i|t_{i-N+1}^{i-1}) = \\frac{c(t_{i-N+1}^{i-1}, t_i)}{c(t_{i-N+1}^{i-1})}$$ \n",
    "\n",
    "where:\n",
    "- $T$ is the total number of tags in a corpus\n",
    "- $c(x)$ is the count of occurrences of $x$ in a corpus ($x$ could be unigram, bigram, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Estimating Emission Probabilities from Data\n",
    "Similar to *transition probabilities*, *emission probabilities* can be estimated from annotated data counting relative frequencies of observations. Since we assume that probability of a word depends only on its tag, the equation is the following.\n",
    "\n",
    "$$p(w_i|t_i) = \\frac{c(t_i,w_i)}{c(t_i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "*Unknown Words* & *Unknown Word Models* \n",
    "\n",
    "Emission probabilities are subject to data sparseness; thus require handling unknown words. \n",
    "Consequently, we need to estimate probabilities for $p($ `<unk>` $|t_i)$. \n",
    "\n",
    "- We can assume that all tags ($t_i$) have equal probability of emitting `<unk>`; and estimate it as $\\frac{1}{V}$, where $V$ is the size of tag vocabulary.\n",
    "    - i.e. use Additive Smoothing\n",
    "- We can estimate them from data replacing OOV with `<unk>` and computing the probabilities\n",
    "- We can build __Unknown Word Model__ (like in Part-of-Speech Tagging), for instance using:\n",
    "    - word shape (capitalization)\n",
    "    - word class (word, punctuation, number)\n",
    "    - part-of-speech tags (generalize)\n",
    "    - word suffixes (last characters): e.g. suffixes of lengths (1 to 5) (e.g. [Samuelsson (1993)](https://www.aclweb.org/anthology/W93-0420.pdf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Decoding\n",
    "$$t_{1}^{n} \\approx \\arg\\max\\limits_{t_{1}^{n}} \\prod^n_{i=1} p(w_i|t_i)p(t_i|t_{i-N+1}^{i-1})$$\n",
    "\n",
    "| __Model__ | __Equation__                                                                                 |\n",
    "|:----------|:--------------------------------------------------------------------------------------------:|\n",
    "| *unigram* | $$t_{1}^{n} = \\arg\\max\\limits_{t_{1}^{n}} \\prod^n_{i=1} p(w_i|t_i)p(t_i)$$                   |\n",
    "| *bigram*  | $$t_{1}^{n} = \\arg\\max\\limits_{t_{1}^{n}} \\prod^n_{i=1} p(w_i|t_i)p(t_i|t_{i-1})$$           |\n",
    "| *trigram* | $$t_{1}^{n} = \\arg\\max\\limits_{t_{1}^{n}} \\prod^n_{i=1} p(w_i|t_i)p(t_i|t_{i-2}, t_{i-1})$$  |\n",
    "\n",
    "where:\n",
    "- $p(w_i|t_i)$ -- *emission probability*, i.e. of seeing current word given the current tag\n",
    "- $p(t_i|t_{i-n+1}^{i-1})$ -- *transition probability*, i.e. of seeing the current tag given the tags we just saw "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Viterbi Algorithm\n",
    "The decoding algorithm for HMMs is the [Viterbi algorithm](https://en.wikipedia.org/wiki/Viterbi_algorithm) -- an instance of dynamic programming. Bigram version of the algorithm is not difficult to implement (see pseudo-code in [SLP 8.4.5](https://web.stanford.edu/~jurafsky/slp3/8.pdf)); trigram, however, is more complex, and practical taggers incorporate other advanced features. \n",
    "\n",
    "There are numerous implementation available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Maximum Likelihood Estimation (__MLE__)\n",
    "\n",
    "Let's compare *emission probability* to *bigram probability* estimation computation:\n",
    "- Maximum Likelihood Estimation (__MLE__) from frequency counts\n",
    "- suffer from data sparseness:\n",
    "    - smoothing (__+1S__ - add-one smoothing, for simplicity)\n",
    "    - out-of-vocabulary (__OOV__, `<unk>`) word uniform probability estimation\n",
    "\n",
    "|         | __bigram *p*__ | __emission *p*__ |\n",
    "|:--------|:-----------------------|:-------------------------|\n",
    "| __MLE__ | $$p(w_i | w_{i-1}) = \\frac{c(w_{i-1}, w_i)}{c(w_{i-1})}$$ | $$p(w_i|t_i) = \\frac{c(t_i,w_i)}{c(t_i)}$$\n",
    "| __+1S__ | $$p(w_i | w_{i-1}) = \\frac{c(w_{i-1},w_i)+1}{c(w_{i-1})+V}$$ | $$p(w_i|t_i)=\\frac{c(t_i,w_i)+1}{c(t_i)+V}$$\n",
    "| __OOV__ | $$\\frac{1}{V}$$ | $$\\frac{1}{V}$$ \n",
    "\n",
    "In practice this means that we can estimate emission probabilities as ngram probabilities, i.e. using the same functions for counting and smoothing, treating $c(t_i,w_i)$ as $c(w_{i-1},w_i)$, i.e. as `[t_i, w_i]` ngram.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part-of-Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Part-of-speech tagging (POS tagging or PoS tagging or POST), also called grammatical tagging is the process of marking up a word in a text as corresponding to a particular part of speech, based on both its definition and its context.\n",
    "\n",
    "Tag Sets vary from corpus to corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Universal Part of Speech Tags\n",
    "\n",
    "Universal POS-Tag Set represents a simplified and unified set of part-of-speech tags, that was proposed for the standardization across corpora and languages. \n",
    "The number of defined tags varies from 12 ([Petrov et al/Google/NLTK](https://github.com/slavpetrov/universal-pos-tags)) to 17 ([Universal Dependencies/spaCy](https://universaldependencies.org/u/pos/index.html), in *Italics*).\n",
    "\n",
    "\n",
    "\n",
    "| Tag  | Meaning | English Examples |\n",
    "|:-----|:--------|:-----------------|\n",
    "| __Open Class__ |||\n",
    "| NOUN | noun (common and proper) | year, home, costs, time, Africa\n",
    "| VERB | verb (all tenses and modes) | is, say, told, given, playing, would\n",
    "| ADJ  | adjective           | new, good, high, special, big, local\n",
    "| ADV  | adverb              | really, already, still, early, now\n",
    "| *PROPN* | proper noun (split from NOUN) | Africa\n",
    "| *INTJ*  | interjection (split from X) | oh, ouch\n",
    "| __Closed Class__ |||\n",
    "| DET  | determiner, article | the, a, some, most, every, no, which\n",
    "| PRON | pronoun             | he, their, her, its, my, I, us\n",
    "| ADP  | adposition\t(prepositions and postpositions) | on, of, at, with, by, into, under\n",
    "| NUM  | numeral             | twenty-four, fourth, 1991, 14:24\n",
    "| PRT (*PART*) | particles or other function words | at, on, out, over per, that, up, with\n",
    "| CONJ | conjunction         | and, or, but, if, while, although\n",
    "| *AUX* | auxiliary (split from VERB) | have, is, should\n",
    "| *CCONJ*  | coordinating conjunction (splits CONJ) | or, and\n",
    "| *SCONJ*  | subordinating conjunction (splits CONJ) | if, while\n",
    "| __Other__ |||\n",
    "| .    | punctuation marks   | . , ; !\n",
    "| X    | other               | foreign words, typos, abbreviations: ersatz, esprit, dunno, gr8, univeristy\n",
    "| *SYM* | symbols (split from X) | $, :) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Part-of-Speech Tagging with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /Users/mdestro/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/mdestro/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/mdestro/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('universal_tagset')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTokens: ['Oh', '.', 'I', 'have', 'seen', 'a', 'man', 'with', 'a', 'telescope', 'in', 'Antarctica', '.']\n\nPOS (WSJ): [('Oh', 'UH'), ('.', '.'), ('I', 'PRP'), ('have', 'VBP'), ('seen', 'VBN'), ('a', 'DT'), ('man', 'NN'), ('with', 'IN'), ('a', 'DT'), ('telescope', 'NN'), ('in', 'IN'), ('Antarctica', 'NNP'), ('.', '.')]\n\nPOS (universal): [('Oh', 'X'), ('.', '.'), ('I', 'PRON'), ('have', 'VERB'), ('seen', 'VERB'), ('a', 'DET'), ('man', 'NOUN'), ('with', 'ADP'), ('a', 'DET'), ('telescope', 'NOUN'), ('in', 'ADP'), ('Antarctica', 'NOUN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "text = \"Oh. I have seen a man with a telescope in Antarctica.\"\n",
    "\n",
    "# tokenization\n",
    "tokens = nltk.word_tokenize(text)\n",
    "print('\\nTokens:', tokens)\n",
    "\n",
    "# POS-tagging (with WSJ Tags)\n",
    "print('\\nPOS (WSJ):', nltk.pos_tag(tokens))\n",
    "\n",
    "# POS-tagging with Universal Tags\n",
    "print('\\nPOS (universal):', nltk.pos_tag(tokens, tagset='universal'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Part-of-Speech Tagging with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['tagger', 'parser', 'ner']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# let's print spaCy pipeline\n",
    "print([key for key, model in nlp.pipeline])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Oh', '.', 'I', 'have', 'seen', 'a', 'man', 'with', 'a', 'telescope', 'in', 'Antarctica', '.']\n['UH', '.', 'PRP', 'VBP', 'VBN', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'NNP', '.']\n['INTJ', 'PUNCT', 'PRON', 'AUX', 'VERB', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'PROPN', 'PUNCT']\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "\n",
    "# tokens\n",
    "print([t.text for t in doc])\n",
    "\n",
    "# Fine grained POS-tags (not universal)\n",
    "print([t.tag_ for t in doc])\n",
    "\n",
    "# Coarse POS-tags (from Universal POS Tag set)\n",
    "print([t.pos_ for t in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training POS-Tagger with NLTK\n",
    "\n",
    "- Manually POS-tagged corpus\n",
    "- Sequence Labeling (Tagging) Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Corpora for POS-Tagging\n",
    "NLTK provides several corpora, most of them are POS-tagged. We will use WSJ with universal tag set (automatically converted using intetnal mapping)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]]\n[[('Pierre', 'NOUN'), ('Vinken', 'NOUN'), (',', '.'), ('61', 'NUM'), ('years', 'NOUN'), ('old', 'ADJ'), (',', '.'), ('will', 'VERB'), ('join', 'VERB'), ('the', 'DET'), ('board', 'NOUN'), ('as', 'ADP'), ('a', 'DET'), ('nonexecutive', 'ADJ'), ('director', 'NOUN'), ('Nov.', 'NOUN'), ('29', 'NUM'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import treebank\n",
    "\n",
    "# WSJ POS-Tags\n",
    "print(treebank.tagged_sents()[:1])\n",
    "\n",
    "# Universal POS-Tags\n",
    "print(treebank.tagged_sents(tagset='universal')[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### NLTK Taggers (Some have also NER)\n",
    "\n",
    "NLTK provides several tagging algorithms, including \n",
    "\n",
    "- rule-based taggers\n",
    "    - Regular Expression Tagger: assigns tags to tokens by comparing their word strings to a series of regular expressions.\n",
    "\n",
    "- [Pre-Trained Taggers](http://www.nltk.org/api/nltk.tag.html)\n",
    "    - HunPoS\n",
    "    - Senna\n",
    "    - Stanford Tagger\n",
    "    \n",
    "- trainable taggers\n",
    "    - `Brill Tagger`: Brill's transformational rule-based tagger assigns an initial tag sequence to a text; and then appies an ordered list of transformational rules to correct the tags of individual tokens. Learns rules from corpus.\n",
    "    - [Greedy Averaged Perceptron](https://explosion.ai/blog/part-of-speech-pos-tagger-in-python)\n",
    "    - [TnT](http://acl.ldc.upenn.edu/A/A00/A00-1031.pdf)\n",
    "    - Hidden Markov Models\n",
    "    - Conditional Random Fields (*later, another lab session*)\n",
    "    - Sequential:\n",
    "        - Affix Tagger: A tagger that chooses a token's tag based on a leading or trailing substring of its word string.\n",
    "        - Ngram Tagger: A tagger that chooses a token's tag based on its word string and on the preceding _n_ word's tags.\n",
    "            - Unigram Tagger\n",
    "            - Bigram Tagger\n",
    "            - Trigram Tagger\n",
    "\n",
    "        - Classifier-based POS Tagger: A sequential tagger that uses a classifier to choose the tag for each token in a sentence.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Testing a POS Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total: 3914; Train: 3132; Test: 782\n"
     ]
    }
   ],
   "source": [
    "# Prepare Training & Test Splits as 80%/20%\n",
    "import math\n",
    "\n",
    "total_size = len(treebank.tagged_sents())\n",
    "train_indx = math.ceil(total_size * 0.8)\n",
    "trn_data = treebank.tagged_sents(tagset='universal')[:train_indx]\n",
    "tst_data = treebank.tagged_sents(tagset='universal')[train_indx:]\n",
    " \n",
    "print(\"Total: {}; Train: {}; Test: {}\".format(total_size, len(trn_data), len(tst_data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INPUT: ['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.']\n",
      "TAG  : [('Pierre', 'NOUN'), ('Vinken', 'NOUN'), (',', '.'), ('61', 'NUM'), ('years', 'NOUN'), ('old', 'NOUN'), (',', '.'), ('will', 'NOUN'), ('join', 'NOUN'), ('the', 'DET'), ('board', 'NOUN'), ('as', 'NOUN'), ('a', 'DET'), ('nonexecutive', 'NOUN'), ('director', 'NOUN'), ('Nov.', 'NOUN'), ('29', 'NUM'), ('.', '.')]\n",
      "Accuracy: 0.5360\n"
     ]
    }
   ],
   "source": [
    "# rule-based tagging\n",
    "from nltk.tag import RegexpTagger\n",
    "\n",
    "# rule from NLTK adapted to Universal Tag Set & extended\n",
    "rules = [\n",
    "    (r'^-?[0-9]+(.[0-9]+)?$', 'NUM'),   # cardinal numbers\n",
    "    (r'(The|the|A|a|An|an)$', 'DET'),   # articles\n",
    "    (r'.*able$', 'ADJ'),                # adjectives\n",
    "    (r'.*ness$', 'NOUN'),               # nouns formed from adjectives\n",
    "    (r'.*ly$', 'ADV'),                  # adverbs\n",
    "    (r'.*s$', 'NOUN'),                  # plural nouns\n",
    "    (r'.*ing$', 'VERB'),                # gerunds\n",
    "    (r'.*ed$', 'VERB'),                 # past tense verbs\n",
    "    (r'[\\.,!\\?:;\\'\"]', '.'),            # punctuation (extension) \n",
    "    (r'.*', 'NOUN')                     # nouns (default)\n",
    "]\n",
    "\n",
    "re_tagger = RegexpTagger(rules)\n",
    "\n",
    "# tagging sentences in test set\n",
    "for s in treebank.sents()[:train_indx]:\n",
    "    print(\"INPUT: {}\".format(s))\n",
    "    print(\"TAG  : {}\".format(re_tagger.tag(s)))\n",
    "    break\n",
    "    \n",
    "# evaluation\n",
    "accuracy = re_tagger.evaluate(tst_data)\n",
    "\n",
    "print(\"Accuracy: {:6.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Exercise (Optional)\n",
    "\n",
    "- Extend rule-set of RegexpTagger to handle close-class words (similar to punctuation & DET):\n",
    "\n",
    "    - prepositions (ADP)\n",
    "    - particles (PRT)\n",
    "    - pronouns (PRON)\n",
    "    - conjunctions (CONJ)\n",
    "\n",
    "- Evaluate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Training HMM POS Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INPUT: ['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.']\n",
      "TAG  : [('Pierre', 'NOUN'), ('Vinken', 'NOUN'), (',', '.'), ('61', 'NUM'), ('years', 'NOUN'), ('old', 'ADJ'), (',', '.'), ('will', 'VERB'), ('join', 'VERB'), ('the', 'DET'), ('board', 'NOUN'), ('as', 'ADP'), ('a', 'DET'), ('nonexecutive', 'ADJ'), ('director', 'NOUN'), ('Nov.', 'NOUN'), ('29', 'NUM'), ('.', '.')]\n",
      "PATH : ['NOUN', 'NOUN', '.', 'NUM', 'NOUN', 'ADJ', '.', 'VERB', 'VERB', 'DET', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'NOUN', 'NUM', '.']\n",
      "Accuracy: 0.5135\n"
     ]
    }
   ],
   "source": [
    "# training hmm on treebank\n",
    "import nltk.tag.hmm as hmm\n",
    "\n",
    "hmm_model = hmm.HiddenMarkovModelTrainer()\n",
    "hmm_tagger = hmm_model.train(trn_data)\n",
    "\n",
    "# tagging sentences in test set\n",
    "for s in treebank.sents()[:train_indx]:\n",
    "    print(\"INPUT: {}\".format(s))\n",
    "    print(\"TAG  : {}\".format(hmm_tagger.tag(s)))\n",
    "    print(\"PATH : {}\".format(hmm_tagger.best_path(s)))\n",
    "    break\n",
    "    \n",
    "# evaluation\n",
    "accuracy = hmm_tagger.evaluate(tst_data)\n",
    "\n",
    "print(\"Accuracy: {:6.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise: Tagging with NLTK\n",
    "Experiment with different taggers provided in NLTK (e.g. NgramTagger)\n",
    "- Explore and experiment with different tagger parameters\n",
    "    - some of them have *cut-off*\n",
    "- For each report evaluation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Shallow Parsing\n",
    "\n",
    "As we have already mentioned, [Shallow Parsing](https://en.wikipedia.org/wiki/Shallow_parsing) is a kind of Sequence Labeling. The main difference from Sequence Labeling task, such as Part-of-Speech tagging, where there is an output label (tag) per token; Shallow Parsing additionally performs __chunking__ -- segmentation of input sequence into constituents. Chunking is required to identify categories (or types) of *multi-word expressions*.\n",
    "In other words, we want to be able to capture information that expressions like `\"New York\"` that consist of 2 tokens, constitute a single unit.\n",
    "\n",
    "What this means in practice is that Shallow Parsing performs *jointly* (or not) 2 tasks:\n",
    "- __Segmentation__ of input into constituents (__spans__)\n",
    "- __Classification__ (Categorization, Labeling) of these constituents into predefined set of labels (__types__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Revisiting Joint Probability Factorization\n",
    "In [*generative approach*](https://en.wikipedia.org/wiki/Generative_model) to Sequence Labeling we are modeling [joint probability distribution](https://en.wikipedia.org/wiki/Joint_probability_distribution).\n",
    "\n",
    "$$p(w_{1}^{n},t_{1}^{n}) = p(w_1, w_2, ..., w_n, t_1, t_2, ..., t_n)$$ \n",
    "\n",
    "To make the inference tractable, we factor the joint distribution using Chain Rule and apply [conditional independence assumption](https://en.wikipedia.org/wiki/Independence_(probability_theory)).\n",
    "\n",
    "$$P(A,B) = P(B|A)P(A) = P(A|B)P(B)$$\n",
    "\n",
    "It is common to mistakenly assume that $P(A|B) = P(B|A)$, known as [Confusion of the Inverse](https://en.wikipedia.org/wiki/Confusion_of_the_inverse).\n",
    "\n",
    "The relation between $P(A|B)$ and $P(B|A)$ is given by the Bayes Rule:\n",
    "\n",
    "$$P(A|B)=\\frac{P(B|A)P(A)}{P(B)}$$\n",
    "\n",
    "Consequently: \n",
    "\n",
    "$$p(t_{1}^{n}|w_{1}^{n}) = \\frac{p(w_{1}^{n},t_{1}^{n})}{p(w_{1}^{n})} = \\frac{p(w_{1}^{n} | t_{1}^{n}) p(t_{1}^{n})}{p(w_{1}^{n})}$$\n",
    "\n",
    "If events $A$ and $B$ are conditionally independents, we have: \n",
    "\n",
    "$$P(A,B) = P(A)P(B) \\rightarrow P(A) = P(A|B); P(B) = P(B|A)$$\n",
    "\n",
    "Applying, Markov assumption to $p(t_{1}^{n})$ and conditional independence assumption to $p(w_{1}^{n} | t_{1}^{n})$ we end-up with our ngram sequence labeling model.\n",
    "\n",
    "\n",
    "$$p(t_{1}^{n}|w_{1}^{n}) \\approx \\prod^n_{i=1} p(w_i|t_i)p(t_i|t_{i-N+1}^{i-1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we would not apply conditional independence assumption to $p(w_{1}^{n}|t_{1}^{n})$, we would be modeling $p(w_{1}^{n},t_{1}^{n})$ __jointly__.\n",
    "\n",
    "Applying just Markov assumption, i.e. modeling it as an ngram (Markov Chain), we will be solving the following equation:\n",
    "\n",
    "$$p(w_{1}^n,t_{1}^{n}) \\approx \\prod_{i=1}^{n}{p(w_{i},t_{i}|w_{i-N+1}^{i-1},t_{i-N+1}^{i-1})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Because:\n",
    "\n",
    "$$p(t_{1}^{n}|w_{1}^{n}) = \\frac{p(w_{1}^{n},t_{1}^{n})}{p(w_{1}^{n})} = \\frac{p(w_{1}^{n} | t_{1}^{n}) p(t_{1}^{n})}{p(w_{1}^{n})}$$\n",
    "\n",
    "$$t_{1}^{n} = \\arg\\max \\limits_{t_{1}^{n}} p(t_{1}^{n}|w_{1}^{n}) = \\arg\\max \\limits_{t_{1}^{n}} p(w_{1}^{n},t_{1}^{n}) = \\arg\\max \\limits_{t_{1}^{n}} p(w_{1}^{n} | t_{1}^{n}) p(t_{1}^{n})$$\n",
    "\n",
    "Most probable sequence can be obtained either way.\n",
    "\n",
    "$$t_{1}^{n} \\approx \\arg\\max\\limits_{t_{1}^{n}} \\prod^n_{i=1} p(w_i|t_i)p(t_i|t_{i-N+1}^{i-1})$$ \n",
    "$$t_{1}^{n} \\approx \\arg\\max\\limits_{t_{1}^{n}} \\prod_{i=1}^{n}{p(w_{i},t_{i}|w_{i-N+1}^{i-1},t_{i-N+1}^{i-1})}$$\n",
    "\n",
    "Factorization and conditional independence assumptions reduce computational complexity and requirements; thus, the *amount of observations* needed to estimate model probabilities.\n",
    "\n",
    "Both models are applied to Shallow Parsing and Sequence Labeling in general:\n",
    "e.g. Hidden Markov Model Tagger and Stochastic Conceptual Language Models for Spoken Language Understanding in [Raymond & Riccardi (2007)](https://disi.unitn.it/~riccardi/papers2/IS07-GenerDiscrSLU.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Joint Segmentation and Classification\n",
    "In Shallow Parsing, the segmentation and label information is generally modeled *jointly*. \n",
    "In practice, it means that our output labels ($t_i$) can be decomposed into ($c_i,s_i$), where $c_i$ is classification label for token $i$, and $s_i$ segmentation label for token $i$.\n",
    "\n",
    "Consequently, in shallow parsing, we are modeling:\n",
    "\n",
    "$$p(w_{1}^{n},t_{1}^{n}) \\rightarrow p(w_{1}^{n},c_{1}^{n},s_{1}^{n}) \\approx \\prod^n_{i=1} p(w_i|c_i,s_i)p(c_i,s_i|c_{i-N+1}^{i-1},s_{i-N+1}^{i-1})$$\n",
    "\n",
    "The joint modeling implies that we do not make conditional independence assumption between segmentation and classification labels. If we make an assumption that probability of a words depends on segmentation and classification labels independently, while both labels depend on their previous N labels, we can factorize the equation as:\n",
    "\n",
    "$$p(w_{1}^{n},c_{1}^{n},s_{1}^{n}) \\approx \\prod^n_{i=1} p(w_i|c_i)p(c_i|c_{i-N+1}^{i-1})p(w_i|s_i)p(s_i|s_{i-N+1}^{i-1})$$\n",
    "\n",
    "The *events* could be modeled independently as well: i.e. we can predict either classification labels only, or segmentation labels only.\n",
    "\n",
    "*Segmentation*:\n",
    "$$p(w_{1}^{n},s_{1}^{n}) \\approx \\prod^n_{i=1} p(w_i|s_i)p(s_i|s_{i-N+1}^{i-1})$$\n",
    "*Classification*\n",
    "$$p(w_{1}^{n},c_{1}^{n}) \\approx \\prod^n_{i=1} p(w_i|c_i)p(c_i|c_{i-N+1}^{i-1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Joint Modeling for Features\n",
    "In Shallow Parsing we jointly model *output label*.\n",
    "The principles of joint modeling could be applied to introduce additional features for *input tokens* as well. \n",
    "For instance, we could model jointly words and part-of-speech tags ($x_i$) for shallow parsing as:\n",
    "\n",
    "$$p(w_{1}^{n},x_{1}^{n},c_{1}^{n},s_{1}^{n}) \\approx \\prod^n_{i=1} p(w_i,x_i|c_i,s_i)p(c_i,s_i|c_{i-N+1}^{i-1},s_{i-N+1}^{i-1})$$\n",
    "\n",
    "or predict them jointly with segmentation and classification labels as:\n",
    "\n",
    "$$p(w_{1}^{n},x_{1}^{n},c_{1}^{n},s_{1}^{n}) \\approx \\prod^n_{i=1} p(w_i|c_i,s_i,x_i)p(c_i,s_i,x_i|c_{i-N+1}^{i-1},s_{i-N+1}^{i-1},x_{i-N+1}^{i-1})$$\n",
    "\n",
    "In the first case our input is *word-pos* pairs, we don't make independence assumptions, consequently they are treated as a single unit (i.e. you need to generate *pos* per word some other way for tagging). Same applies to *segmentation-classification* (or *segmentation-classification-pos*) output labels.\n",
    "\n",
    "- In joint modeling our observations for tokens and ngrams are more sparse: *word-pos* pair usually appears in data less than *word* and *pos* separately (same applies for their ngrams). \n",
    "- In joint modeling of output labels, we will have to estimate more of them, thus will have less observations for each.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### [Bayesian Categorization](https://en.wikipedia.org/wiki/Naive_Bayes_classifier)\n",
    "\n",
    "Assuming conditional independence between *word* and *pos* leads to Bayesian Categorization.\n",
    "\n",
    "$$p(C_k|x) = \\frac{p(x|C_k)p(C_k)}{p(x)} = p(C_k) \\prod^n_{i=1} p(x_i|C_k)$$\n",
    "\n",
    "$$p(t_{i}|w_{i},x_{i}) \\approx p(t_{i})p(w_i|t_i)p(x_i|t_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Encoding Segmentation Information: CoNLL Corpus Format\n",
    "\n",
    "Corpus in CoNLL format consists of series of sentences, separated by blank lines. Each sentence is encoded using a table (or \"grid\") of values, where each line corresponds to a single word, and each column corresponds to an annotation type. \n",
    "\n",
    "The set of columns used by CoNLL-style files can vary from corpus to corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```\n",
    "who    O\n",
    "plays  O\n",
    "luke   B-character.name\n",
    "on     O\n",
    "star   B-movie.name\n",
    "wars   I-movie.name\n",
    "new    I-movie.name\n",
    "hope   I-movie.name\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### [IOB Scheme](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging))\n",
    "\n",
    "- The notation scheme is used to label *multi-word* spans in token-per-line format.\n",
    "    - *star wars new hope* is a *movie.name* concept that has 4 tokens\n",
    "- Both, prefix and suffix notations are commons: \n",
    "    - prefix: __B-movie.name__\n",
    "    - suffix: __movie.name-B__\n",
    "\n",
    "- Meaning of Prefixes\n",
    "    - __B__ for (__B__)eginning of span\n",
    "    - __I__ for (__I__)nside of span\n",
    "    - __O__ for (__O__)tside of span (no prefix or suffix, just `O`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Alternative Schemes:\n",
    "- No prefix or suffix (useful when there are no *multi-word* concepts)\n",
    "```\n",
    "        who    O\n",
    "        plays  O\n",
    "        luke   character.name\n",
    "        on     O\n",
    "        star   movie.name\n",
    "        wars   movie.name\n",
    "        new    movie.name\n",
    "        hope   movie.name\n",
    "```\n",
    "- __IOB/IOB2/BIO__\n",
    "\n",
    "- __IOBE__\n",
    "    - IOB + \n",
    "    - __E__ for (__E__)nd of span (or __L__ for (__L__)ast)\n",
    "```\n",
    "        who    O\n",
    "        plays  O\n",
    "        luke   B-character.name\n",
    "        on     O\n",
    "        star   B-movie.name\n",
    "        wars   I-movie.name\n",
    "        new    I-movie.name\n",
    "        hope   E-movie.name\n",
    "```\n",
    "    \n",
    "- __BILOU/BIOES__\n",
    "    - IOB + \n",
    "    - __L__ for (__L__)ast word of span\n",
    "    - __U__ for (__U__)nit word (or __S__ for (__S__)ingleton)\n",
    "```\n",
    "        who    O\n",
    "        plays  O\n",
    "        luke   U-character.name\n",
    "        on     O\n",
    "        star   B-movie.name\n",
    "        wars   I-movie.name\n",
    "        new    I-movie.name\n",
    "        hope   L-movie.name\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Choice of Scheme\n",
    "- It is possible to convert IOB, IOBE, & BILOU formats to each other\n",
    "- Each prefix is applied to every concept label, consequently we increase the number of transitions whose probabilities we need to estimate; \n",
    "    - increasing data sparseness, as for each label we will have less observations\n",
    "- The choice of scheme depends on the amount of available data:\n",
    "    - __IOB__ for least amount\n",
    "    - __BILOU__ for the most amount "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Terminology\n",
    "There is no strict naming convention regarding schemes (see alternatives) or how each constituent is termed. \n",
    "Below is the terminology used in this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```\n",
    "    who    O\n",
    "    plays  O\n",
    "    luke   B-character.name\n",
    "    on     O\n",
    "    star   B-movie.name\n",
    "    wars   I-movie.name\n",
    "    new    I-movie.name\n",
    "    hope   I-movie.name\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Interpretation\n",
    "Segmentation and Labeling data formats encode the following information:\n",
    "- in string (sentence) `\"who plays luke on star wars new hope\"`\n",
    "- there are 2 __entities__ (a.k.a. chunks, concepts or slots, depending on NLP task and perspective), that have __types__ (labels)\n",
    "    - `character.name`\n",
    "    - `movie.name`\n",
    "    \n",
    "- entity of __type__ `movie.name`: \n",
    "    - has __span__:\n",
    "        - as tokens from `0` for *CoNLL*: `[5:7]`\n",
    "    - has __value__: `\"star wars new hope\"`\n",
    "        - string *covered by* (*on included*) in __span__\n",
    " \n",
    "*CoNLL* format encodes __tokenization__ informations. In other words, how string `\"star wars new hope\"` is split into tokens. Since most Sequence Labeling algorithms operate on token level, internally the strings are split into tokens, applying *IOB*-like schemes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Named Entity Recognition with NLTK\n",
    "[NLTK](https://www.nltk.org/api/nltk.tag.html) provides implementations of popular sequence labeling algorithms for Part-of-Speech Tagging (including [HMM](https://www.nltk.org/api/nltk.tag.html#module-nltk.tag.hmm)), that can be used for Sequence Labeling in general. \n",
    "\n",
    "- Loading & working with CoNLL format corpora in NLTK\n",
    "- Tagger training & testing (running)\n",
    "\n",
    "To have a custom tagger that labels input text with our __custom label set__, we need to __train__ it on a corpus annotated with this __custom label set__.\n",
    "\n",
    "Addtionally, NLTK provides [Chunking](http://www.nltk.org/api/nltk.chunk.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### NLTK Pre-trained NE Chunker\n",
    "\n",
    "NLTK provides a classifier that has already been trained to recognize named entities, accessed with the function `nltk.ne_chunk()`. If we set the parameter `binary=True`, then named entities are just tagged as `NE`; otherwise, the classifier adds category labels such as `PERSON`, `ORGANIZATION`, and `GPE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/mdestro/nltk_data...\n",
      "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to /Users/mdestro/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n",
      "(S\n",
      "  (PERSON Pierre/NNP)\n",
      "  (ORGANIZATION Vinken/NNP)\n",
      "  ,/,\n",
      "  61/CD\n",
      "  years/NNS\n",
      "  old/JJ\n",
      "  ,/,\n",
      "  will/MD\n",
      "  join/VB\n",
      "  the/DT\n",
      "  board/NN\n",
      "  as/IN\n",
      "  a/DT\n",
      "  nonexecutive/JJ\n",
      "  director/NN\n",
      "  Nov./NNP\n",
      "  29/CD\n",
      "  ./.)\n",
      "(S\n",
      "  (NE Pierre/NNP Vinken/NNP)\n",
      "  ,/,\n",
      "  61/CD\n",
      "  years/NNS\n",
      "  old/JJ\n",
      "  ,/,\n",
      "  will/MD\n",
      "  join/VB\n",
      "  the/DT\n",
      "  board/NN\n",
      "  as/IN\n",
      "  a/DT\n",
      "  nonexecutive/JJ\n",
      "  director/NN\n",
      "  Nov./NNP\n",
      "  29/CD\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "for s in treebank.tagged_sents():\n",
    "    print(s)\n",
    "    print(nltk.ne_chunk(s))\n",
    "    print(nltk.ne_chunk(s, binary=True))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training NLTK Taggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package conll2002 to\n",
      "[nltk_data]     /Users/mdestro/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/conll2002.zip.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "\n",
    "nltk.download('conll2002')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "35651\n('LOC', 'PER', 'ORG', 'MISC')\n['Melbourne', '(', 'Australia', ')', ',', '25', 'may', '(', 'EFE', ')', '.']\n[('Melbourne', 'NP'), ('(', 'Fpa'), ('Australia', 'NP'), (')', 'Fpt'), (',', 'Fc'), ('25', 'Z'), ('may', 'NC'), ('(', 'Fpa'), ('EFE', 'NC'), (')', 'Fpt'), ('.', 'Fp')]\n(S\n  (LOC Melbourne/NP)\n  (/Fpa\n  (LOC Australia/NP)\n  )/Fpt\n  ,/Fc\n  25/Z\n  may/NC\n  (/Fpa\n  (ORG EFE/NC)\n  )/Fpt\n  ./Fp)\n[('Melbourne', 'NP', 'B-LOC'), ('(', 'Fpa', 'O'), ('Australia', 'NP', 'B-LOC'), (')', 'Fpt', 'O'), (',', 'Fc', 'O'), ('25', 'Z', 'O'), ('may', 'NC', 'O'), ('(', 'Fpa', 'O'), ('EFE', 'NC', 'B-ORG'), (')', 'Fpt', 'O'), ('.', 'Fp', 'O')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import conll2002\n",
    "\n",
    "print(len(conll2002.tagged_sents()))\n",
    "print(conll2002._chunk_types)\n",
    "print(conll2002.sents('esp.train')[0])\n",
    "print(conll2002.tagged_sents('esp.train')[0])\n",
    "print(conll2002.chunked_sents('esp.train')[0])\n",
    "print(conll2002.iob_sents('esp.train')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('Melbourne', 'NP', 'B-LOC'), ('(', 'Fpa', 'O'), ('Australia', 'NP', 'B-LOC'), (')', 'Fpt', 'O'), (',', 'Fc', 'O'), ('25', 'Z', 'O'), ('may', 'NC', 'O'), ('(', 'Fpa', 'O'), ('EFE', 'NC', 'B-ORG'), (')', 'Fpt', 'O'), ('.', 'Fp', 'O')]\n",
      "[('Melbourne', 'B-LOC'), ('(', 'O'), ('Australia', 'B-LOC'), (')', 'O'), (',', 'O'), ('25', 'O'), ('may', 'O'), ('(', 'O'), ('EFE', 'B-ORG'), (')', 'O'), ('.', 'O')]\n",
      "Accuracy: 0.3760\n"
     ]
    }
   ],
   "source": [
    "# training hmm on training data: exactly as above\n",
    "import nltk.tag.hmm as hmm\n",
    "\n",
    "hmm_model = hmm.HiddenMarkovModelTrainer()\n",
    "\n",
    "print(conll2002.iob_sents('esp.train')[0])\n",
    "\n",
    "# let's get only word and iob-tag\n",
    "trn_sents = [[(text, iob) for text, pos, iob in sent] for sent in conll2002.iob_sents('esp.train')]\n",
    "print(trn_sents[0])\n",
    "\n",
    "tst_sents = [[(text, iob) for text, pos, iob in sent] for sent in conll2002.iob_sents('esp.testa')]\n",
    "\n",
    "hmm_ner = hmm_model.train(trn_sents)\n",
    "    \n",
    "# evaluation\n",
    "accuracy = hmm_ner.evaluate(tst_sents)\n",
    "\n",
    "print(\"Accuracy: {:6.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### NLTK Chunk Tagger Note\n",
    "HMM uses only words as input, NLTK also povides trainable MaxEnt Chunker Tagger, which unfortunatelly requires `megam` file. Unfortunatelly, it is very convoluted to install. (http://www.umiacs.umd.edu/~hal/megam/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Segmentation \n",
    "Train a tagger to perform *segmentation* of input sentences into constituents\n",
    "- Strip concept information from output labels (i.e. keep only IOB-prefix)\n",
    "- Train tagger to predict segmentation labels\n",
    "- Evaluate segmentation performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### CoNLL Eval: Exercise\n",
    "CoNLL Community developed a perl script to evaluate *segmentation* and *labeling* performance jointly using IOB information. Such evaluation provides more accurate assessment of the shallow parsing performance, in comparison to token-level metrics (e.g. NLTK accuracy).\n",
    "\n",
    "- import `evaluate` function from `conll.py` (example shown)\n",
    "- evaluate tagger predictions\n",
    "- compare performances to token-level accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('La', 'B-LOC'), ('Coruña', 'I-LOC'), (',', 'O'), ('23', 'O'), ('may', 'O'), ('(', 'O'), ('EFECOM', 'B-ORG'), (')', 'O'), ('.', 'O')]\n",
      "[('La', 'B-LOC'), ('Coruña', 'I-LOC'), (',', 'O'), ('23', 'O'), ('may', 'O'), ('(', 'O'), ('EFECOM', 'B-ORG'), (')', 'O'), ('.', 'O')]\n",
      "{'PER': {'p': 0.6912751677852349, 'r': 0.2802721088435374, 'f': 0.398838334946757, 's': 735}, 'LOC': {'p': 0.0297753899928798, 'r': 0.8487084870848709, 'f': 0.0575323619535989, 's': 1084}, 'ORG': {'p': 0.7948350071736011, 'r': 0.39571428571428574, 'f': 0.5283738674296614, 's': 1400}, 'MISC': {'p': 0.5702479338842975, 'r': 0.20294117647058824, 'f': 0.299349240780911, 's': 340}, 'total': {'p': 0.05463234834759793, 'r': 0.4914301770160157, 'f': 0.09833300536924071, 's': 3559}}\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "           p      r      f     s\n",
       "PER    0.691  0.280  0.399   735\n",
       "LOC    0.030  0.849  0.058  1084\n",
       "ORG    0.795  0.396  0.528  1400\n",
       "MISC   0.570  0.203  0.299   340\n",
       "total  0.055  0.491  0.098  3559"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>p</th>\n      <th>r</th>\n      <th>f</th>\n      <th>s</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>PER</th>\n      <td>0.691</td>\n      <td>0.280</td>\n      <td>0.399</td>\n      <td>735</td>\n    </tr>\n    <tr>\n      <th>LOC</th>\n      <td>0.030</td>\n      <td>0.849</td>\n      <td>0.058</td>\n      <td>1084</td>\n    </tr>\n    <tr>\n      <th>ORG</th>\n      <td>0.795</td>\n      <td>0.396</td>\n      <td>0.528</td>\n      <td>1400</td>\n    </tr>\n    <tr>\n      <th>MISC</th>\n      <td>0.570</td>\n      <td>0.203</td>\n      <td>0.299</td>\n      <td>340</td>\n    </tr>\n    <tr>\n      <th>total</th>\n      <td>0.055</td>\n      <td>0.491</td>\n      <td>0.098</td>\n      <td>3559</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 114
    }
   ],
   "source": [
    "# to import conll\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('../src/'))\n",
    "\n",
    "from conll import evaluate\n",
    "# for nice tables\n",
    "import pandas as pd\n",
    "\n",
    "# getting references (note that it is testb this time)\n",
    "refs = [[(text, iob) for text, pos, iob in sent] for sent in conll2002.iob_sents('esp.testb')]\n",
    "print(refs[0])\n",
    "# getting hypotheses\n",
    "hyps = [hmm_ner.tag(s) for s in conll2002.sents('esp.testb')]\n",
    "print(hyps[0])\n",
    "\n",
    "results = evaluate(refs, hyps)\n",
    "\n",
    "pd_tbl = pd.DataFrame().from_dict(results, orient='index')\n",
    "pd_tbl.round(decimals=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Named Entity Recognition with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Pierre Vinken', '61 years old', 'Nov. 29']\n[('PERSON', 'B'), ('PERSON', 'I'), ('', 'O'), ('DATE', 'B'), ('DATE', 'I'), ('DATE', 'I'), ('', 'O'), ('', 'O'), ('', 'O'), ('', 'O'), ('', 'O'), ('', 'O'), ('', 'O'), ('', 'O'), ('', 'O'), ('DATE', 'B'), ('DATE', 'I'), ('', 'O')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "txt = 'Pierre Vinken, 61 years old, will join the board as a nonexecutive director Nov. 29.'\n",
    "doc = nlp(txt)\n",
    "\n",
    "print([ent.text for ent in doc.ents])\n",
    "print([(t.ent_type_, t.ent_iob_) for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise:\n",
    "- Evaluate spaCy NER model using CoNLL evaluation script on CoNLL 2002 Test B data\n",
    "- Data is Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Assignment\n",
    "\n",
    "Assigment is in the intersection of Named Entity Recognition and Dependency Parsing.\n",
    "\n",
    "0. Evaluate spaCy NER on CoNLL 2003 data (provided)\n",
    "    - report token-level performance (per class and total)\n",
    "        - accuracy of correctly recognizing all tokens that belong to named entities (i.e. tag-level accuracy) \n",
    "    - report CoNLL chunk-level performance (per class and total);\n",
    "        - precision, recall, f-measure of correctly recognizing all the named entities in a chunk per class and total  \n",
    "\n",
    "1. Grouping of Entities.\n",
    "Write a function to group recognized named entities using `noun_chunks` method of [spaCy](https://spacy.io/usage/linguistic-features#noun-chunks). Analyze the groups in terms of most frequent combinations (i.e. NER types that go together). \n",
    "\n",
    "2. One of the possible post-processing steps is to fix segmentation errors.\n",
    "Write a function that extends the entity span to cover the full noun-compounds. Make use of `compound` dependency relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "name": "python383jvsc74a57bd0ae03d948584253fec240bc08d589a6fc5490ee03a1d28d126cf31e5ae02e89c9",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}