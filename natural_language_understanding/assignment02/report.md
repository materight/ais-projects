# Natural Language Understanding - Assignment 2 Report
Student: **Destro Matteo (221222)**

## Exercise 1
The first exercise asked to evaluate the performance of spaCy NER on the CoNLL 2003 dataset. Before computing the actual evaluation, a preprocessing of the doc generated by spaCy was necessary to align its tokens to the format available in the dataset. 

In particular, two kind of misalignment were addressed, regarding:
- *tokens*: sometime spaCy may split a token into two parts, while in the CoNLL dataset the same token may be considered as a single one, e.g. "AL-AIN".
- *sentences*: the sentences split of spaCy may be different from the one in the dataset, since the dataset provides splits based also on the meaning of a sentence, e.g. titles or score reports.

To solve the first issue, the corpus was given in input to spaCy in the form of an array of words, to force the tokenization format of the datasets. For the second issue, the token's property `is_sent_start` was exploited to set manually the sentences boundaries, i.e. by setting it to true only for the first word of each sentence in the original dataset, and false for every other word.

Another issue addressed was the format of the named entity labels given by spaCy, that did not correspond to the ones used in the dataset. A simple mapping between the two sets of labels was enough to solve it. In particular, the label "PERSON" was mapped to "PER" and "GPE" and "FAC" to "LOC". The labels "LOC" and "ORG" remained the same, while any other label was mapped to "MISC".

After this preprocessing step it was possible to compute the token and chunk-level performances. In the first case, the metrics were computed using scikit-learn and taking into consideration both entity type and IOB tag of each token in the spaCy document, and comparing them with the ones provided by the dataset. For the chunk-level performance, the precision, recall and f-measure requested were computed using the `evaluate` function defined in `conll.py`.

## Exercise 2
The second exercise asked to group recognized named entities using the `noun_chunks` property of a spaCy document. A first analysis of the entities contained in each noun_chunk showed that some entities are not grouped. However, they should not be ignored and should be anyway returned in their own chunk. 

To do so, the script iterates over each named entity of every noun_chunk. At the same time, it iterates over the named entities in the parsed document. By doing this, it is possible to identify which entity belongs to a certain chunk without losing the entities that do not belong to any chunk. At each iteration step, the algorithm compare the current entity in the current noun_chunk with the current entity in the parsed document. If they are equal, the entity is added to the current chunk. Otherwise, a new chunk is created and the entity is added to it. Please note that since the algorithm processes each entity one by one, it may take a while to finish (~3 minutes during testing).

## Exercise 3
The third exercise asked to fix possible segmentation errors by including in each entity the tokens that have a `compound` dependency relation with it. To do so, the algorithm iterates over all the entities in the document, and maintains an array of "new entities" that initially contains the default entities returned by spaCy. Then it iterates over all the tokens of the current entity: for each token, it checks if there is a compound dependency relation between it and its parent or children. If such dependency is found, it checks that the linked token does not already belong to another entity and that its position is adjacent to the named entity's boundaries, i.e. that its position `i` is either equal to `entity.start-1` or `entity.end+1`. If both conditions are true, a new `Span` object is created with the new extended entity boundaries, such that the token will be covered by the entity. The check for adjacency was implemented to avoid including other tokens (without a compound relation) that may be  present between the entity and the selected token. The new token is then added to the list of tokens that have to be processed, so that if the token itself has a compound relation with another token, it will also be analyzed as explained previously.

The results however showed that this kind of post-processing lead to worse performance w.r.t the original NER results, at least for this particular text corpus. For example, the token-level precision dropped from an average of 0.883 to 0.874, while the total chunk-level precision went from 0.397 to 0.350. More results can be found in the Jupyter notebook.